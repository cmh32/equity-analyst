  I have a multi-agent equity analysis system that currently runs several specialized agents in parallel:
  - Macro Agent: Analyzes macroeconomic factors affecting a stock
  - Fundamentals Agent: Analyzes company financials, valuation metrics, etc.
  - Chief Investment Officer (CIO): Synthesizes all agent outputs into a final investment memo

  I want to refactor this into a **manager-agent architecture** where a Manager Agent orchestrates, critiques, and iteratively improves the outputs of the other agents before
  the final memo is produced.

  **Current Architecture:**
  - Agents run independently and produce their analysis
  - CIO receives all outputs and writes final memo
  - No feedback loop or quality control

  **Desired Architecture:**
  1. **Manager Agent** that:
     - Assigns tasks to specialist agents (Macro, Fundamentals, etc.)
     - Reviews each agent's output for quality, completeness, and accuracy
     - Provides specific critique and feedback (e.g., "Your valuation analysis is missing DCF comparison", "Macro analysis doesn't address interest rate sensitivity")
     - Can send work back to agents for revision with specific instructions
     - Decides when outputs are "good enough" to proceed
     - Tracks iteration count to prevent infinite loops (max 2-3 revision cycles)

  2. **Specialist Agents** that:
     - Receive initial task + any revision feedback from Manager
     - Produce their analysis
     - Can see the Manager's critique and improve their output

  3. **Final Output:**
     - Same deliverables as before (investment memo, individual analyses)
     - But now includes a "critique trail" showing what was improved
     - Higher quality outputs due to iterative refinement

  **Technical Requirements:**
  - Using Claude API with tool use
  - Python-based (currently using a agent framework with tools for web search, etc.)
  - Want to maintain the FastAPI backend structure
  - Outputs should still be accessible via API for my web frontend

  **Please provide:**
  1. The core architecture/class structure for the Manager Agent
  2. How the Manager should format its critiques and revision requests
  3. The orchestration loop (Manager assigns → Agent produces → Manager critiques → repeat or proceed)
  4. How to preserve and display the critique history in the final output
  5. Example prompts/system messages for the Manager Agent role

  Focus on clean, minimal implementation. I don't need excessive error handling or abstraction - just the core pattern that works.

  ---
  This prompt gives Claude the context about your current system and clear requirements for the manager-agent pattern. Want me to adjust the prompt to emphasize any particular
  aspect (e.g., more focus on the critique format, the API structure, or the agent communication pattern)?

> i still want to use OpenAI API, the search funciton, etc 

⏺ Here's an updated prompt that preserves your existing tech stack:

  ---
  Prompt for Claude:

  I have a multi-agent equity analysis system built with Python that I want to refactor to include a Manager Agent. Here's my current setup:

  **Current Tech Stack (keep all of this):**
  - OpenAI API (GPT models) for agent LLM calls
  - Web search tool for agents to gather real-time information
  - FastAPI backend serving a JavaScript frontend
  - Agents: Macro Agent, Fundamentals Agent, Chief Investment Officer (CIO)

  **Current Flow:**
  - User submits a stock ticker via frontend
  - Agents run and produce their analyses (using OpenAI + search tools)
  - CIO synthesizes everything into a final investment memo
  - Results returned to frontend

  **What I Want to Add - Manager Agent Architecture:**

  1. **Manager Agent** (also uses OpenAI API) that:
     - Orchestrates the specialist agents (Macro, Fundamentals)
     - Reviews each agent's output after they complete
     - Critiques for quality, completeness, accuracy (e.g., "Missing competitor analysis", "Valuation section lacks DCF", "Macro view doesn't address tariff risks")
     - Sends work back to agents with specific revision instructions
     - Decides when outputs meet quality bar (max 2-3 revision cycles)
     - Once satisfied, passes approved outputs to CIO for final memo

  2. **Revised Flow:**
     User Request → Manager assigns tasks
                  → Agents produce analysis (using OpenAI + search)
                  → Manager critiques outputs
                  → If revision needed: Agent revises with feedback
                  → Loop until approved (max 3 iterations)
                  → Approved outputs → CIO writes final memo
                  → Return to frontend

  3. **Same Outputs as Before:**
  - Final investment memo from CIO
  - Individual agent analyses (Macro, Fundamentals)
  - NEW: Include critique/revision history showing improvements made

  **Please provide:**
  1. Manager Agent class/function that integrates with my existing OpenAI API setup
  2. How to modify my existing agent functions to accept revision feedback
  3. The orchestration loop connecting Manager → Agents → Manager → CIO
  4. Manager's system prompt for critiquing financial analysis
  5. How to structure the critique feedback so agents can act on it
  6. Updates needed to my FastAPI endpoint to handle the new flow

  Keep the implementation minimal and practical. Preserve my existing patterns for OpenAI calls and search tool usage - just add the manager layer on top.